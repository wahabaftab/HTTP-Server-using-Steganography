This is text 3
The evolution of datagram and VC networks reflects their origins. The notion of a
virtual circuit as a central organizing principle has its roots in the telephony world,
which uses real circuits. With call setup and per-call state being maintained at the
routers within the network, a VC network is arguably more complex than a datagram network (although see [Molinero-Fernandez 2002] for an interesting comparison of the complexity of circuit- versus packet-switched networks). This, too, is in
keeping with its telephony heritage. Telephone networks, by necessity, had their
complexity within the network, since they were connecting dumb end-system
devices such as rotary telephones. (For those too young to know, a rotary phone is
an analog telephone with no buttons—only a dial.)
The Internet as a datagram network, on the other hand, grew out of the need to
connect computers together. Given more sophisticated end-system devices, the
Internet architects chose to make the network-layer service model as simple as possible. As we have already seen in Chapters 2 and 3, additional functionality (for
example, in-order delivery, reliable data transfer, congestion control, and DNS name
resolution) is then implemented at a higher layer, in the end systems. This inverts
the model of the telephone network, with some interesting consequences:
• Since the resulting Internet network-layer service model makes minimal (no!)
service guarantees, it imposes minimal requirements on the network layer. This
makes it easier to interconnect networks that use very different link-layer technologies (for example, satellite, Ethernet, fiber, or radio) that have very different
transmission rates and loss characteristics. We will address the interconnection
of IP networks in detail in Section 4.4.
4.2 • VIRTUAL CIRCUIT AND DATAGRAM NETWORKS 319
• As we saw in Chapter 2, applications such as e-mail, the Web, and even some
network infrastructure services such as the DNS are implemented in hosts
(servers) at the network edge. The ability to add a new service simply by attaching a host to the network and defining a new application-layer protocol (such as
HTTP) has allowed new Internet applications such as the Web to be deployed in
a remarkably short period of time.
4.3 What’s Inside a Router?
Now that we’ve overviewed the network layer’s services and functions, let’s turn
our attention to its forwarding function—the actual transfer of packets from a
router’s incoming links to the appropriate outgoing links at that router. We
already took a brief look at a few aspects of forwarding in Section 4.2, namely,
addressing and longest prefix matching. We mention here in passing that the terms
forwarding and switching are often used interchangeably by computer-networking
researchers and practitioners; we’ll use both terms interchangeably in this
textbook as well.
A high-level view of a generic router architecture is shown in Figure 4.6. Four
router components can be identified:
• Input ports. An input port performs several key functions. It performs the
physical layer function of terminating an incoming physical link at a router;
this is shown in the leftmost box of the input port and the rightmost box of the
output port in Figure 4.6. An input port also performs link-layer functions
needed to interoperate with the link layer at the other side of the incoming
link; this is represented by the middle boxes in the input and output ports. Perhaps most crucially, the lookup function is also performed at the input port;
this will occur in the rightmost box of the input port. It is here that the forwarding table is consulted to determine the router output port to which an
arriving packet will be forwarded via the switching fabric. Control packets
(for example, packets carrying routing protocol information) are forwarded
from an input port to the routing processor. Note that the term port here—
referring to the physical input and output router interfaces—is distinctly
different from the software ports associated with network applications and
sockets discussed in Chapters 2 and 3.
• Switching fabric. The switching fabric connects the router’s input ports to its
output ports. This switching fabric is completely contained within the router—
a network inside of a network router!
• Output ports. An output port stores packets received from the switching fabric
and transmits these packets on the outgoing link by performing the necessary
link-layer and physical-layer functions. When a link is bidirectional (that is,
320 CHAPTER 4 • THE NETWORK LAYER
carries traffic in both directions), an output port will typically be paired with the
input port for that link on the same line card (a printed circuit board containing
one or more input ports, which is connected to the switching fabric).
• Routing processor. The routing processor executes the routing protocols (which
we’ll study in Section 4.6), maintains routing tables and attached link state information, and computes the forwarding table for the router. It also performs the
network management functions that we’ll study in Chapter 9.
Recall that in Section 4.1.1 we distinguished between a router’s forwarding and
routing functions. A router’s input ports, output ports, and switching fabric
together implement the forwarding function and are almost always implemented
in hardware, as shown in Figure 4.6. These forwarding functions are sometimes
collectively referred to as the router forwarding plane. To appreciate why a
hardware implementation is needed, consider that with a 10 Gbps input link and a
64-byte IP datagram, the input port has only 51.2 ns to process the datagram
before another datagram may arrive. If N ports are combined on a line card (as is
often done in practice), the datagram-processing pipeline must operate N times
faster—far too fast for software implementation. Forwarding plane hardware can
be implemented either using a router vendor’s own hardware designs, or constructed using purchased merchant-silicon chips (e.g., as sold by companies such
as Intel and Broadcom).
While the forwarding plane operates at the nanosecond time scale, a router’s
control functions—executing the routing protocols, responding to attached links that
4.3 • WHAT’S INSIDE A ROUTER? 321
Input port Output port
Input port Output port
Routing
processor
Routing, management
control plane (software)
Forwarding
data plane (hardware)
Switch
fabric
Figure 4.6  Router architecture
go up or down, and performing management functions such as those we’ll study in
Chapter 9—operate at the millisecond or second timescale. These router control
plane functions are usually implemented in software and execute on the routing
processor (typically a traditional CPU).
Before delving into the details of a router’s control and data plane, let’s return to
our analogy of Section 4.1.1, where packet forwarding was compared to cars entering
and leaving an interchange. Let’s suppose that the interchange is a roundabout, and that
before a car enters the roundabout, a bit of processing is required—the car stops at an
entry station and indicates its final destination (not at the local roundabout, but the ultimate destination of its journey). An attendant at the entry station looks up the final destination, determines the roundabout exit that leads to that final destination, and tells the
driver which roundabout exit to take. The car enters the roundabout (which may be
filled with other cars entering from other input roads and heading to other roundabout
exits) and eventually leaves at the prescribed roundabout exit ramp, where it may
encounter other cars leaving the roundabout at that exit.
We can recognize the principal router components in Figure 4.6 in this analogy—the entry road and entry station correspond to the input port (with a lookup
function to determine to local outgoing port); the roundabout corresponds to the
switch fabric; and the roundabout exit road corresponds to the output port. With
this analogy, it’s instructive to consider where bottlenecks might occur. What happens if cars arrive blazingly fast (for example, the roundabout is in Germany or
Italy!) but the station attendant is slow? How fast must the attendant work to ensure
there’s no backup on an entry road? Even with a blazingly fast attendant, what happens if cars traverse the roundabout slowly—can backups still occur? And what
happens if most of the entering cars all want to leave the roundabout at the same
exit ramp—can backups occur at the exit ramp or elsewhere? How should the
roundabout operate if we want to assign priorities to different cars, or block certain
cars from entering the roundabout in the first place? These are all analogous to critical questions faced by router and switch designers.
In the following subsections, we’ll look at router functions in more detail. [Iyer
2008, Chao 2001; Chuang 2005; Turner 1988; McKeown 1997a; Partridge 1998]
provide a discussion of specific router architectures. For concreteness, the ensuing
discussion assumes a datagram network in which forwarding decisions are based
on the packet’s destination address (rather than a VC number in a virtual-circuit
network). However, the concepts and techniques are quite similar for a virtualcircuit network.
4.3.1 Input Processing
A more detailed view of input processing is given in Figure 4.7. As discussed above,
the input port’s line termination function and link-layer processing implement the
physical and link layers for that individual input link. The lookup performed in the
input port is central to the router’s operation—it is here that the router uses the forwarding table to look up the output port to which an arriving packet will be
322 CHAPTER 4 • THE NETWORK LAYER
forwarded via the switching fabric. The forwarding table is computed and updated
by the routing processor, with a shadow copy typically stored at each input port. The
forwarding table is copied from the routing processor to the line cards over a separate bus (e.g., a PCI bus) indicated by the dashed line from the routing processor to
the input line cards in Figure 4.6. With a shadow copy, forwarding decisions can be
made locally, at each input port, without invoking the centralized routing processor
on a per-packet basis and thus avoiding a centralized processing bottleneck.
Given the existence of a forwarding table, lookup is conceptually simple—we just
search through the forwarding table looking for the longest prefix match, as described
4.3 • WHAT’S INSIDE A ROUTER? 323
Line
termination
Data link
processing
(protocol,
decapsulation)
Lookup, fowarding,
queuing Switch
fabric
Figure 4.7  Input port processing
CISCO SYSTEMS: DOMINATING THE NETWORK CORE
As of this writing 2012, Cisco employs more than 65,000 people. How did this
gorilla of a networking company come to be? It all started in 1984 in the living room
of a Silicon Valley apartment.
Len Bosak and his wife Sandy Lerner were working at Stanford University when they
had the idea to build and sell Internet routers to research and academic institutions, the
primary adopters of the Internet at that time. Sandy Lerner came up with the name Cisco
(an abbreviation for San Francisco), and she also designed the company’s bridge logo.
Corporate headquarters was their living room, and they financed the project with credit
cards and moonlighting consulting jobs. At the end of 1986, Cisco’s revenues reached
$250,000 a month. At the end of 1987, Cisco succeeded in attracting venture capital—
$2 million from Sequoia Capital in exchange for one-third of the company. Over the next
few years, Cisco continued to grow and grab more and more market share. At the same
time, relations between Bosak/Lerner and Cisco management became strained. Cisco
went public in 1990; in the same year Lerner and Bosak left the company.
Over the years, Cisco has expanded well beyond the router market, selling security,
wireless caching, Ethernet switch, datacenter infrastructure, video conferencing, and
voice-over IP products and services. However, Cisco is facing increased international
competition, including from Huawei, a rapidly growing Chinese network-gear company. Other sources of competition for Cisco in the router and switched Ethernet space
include Alcatel-Lucent and Juniper.
CASE HISTORY
in Section 4.2.2. But at Gigabit transmission rates, this lookup must be performed in
nanoseconds (recall our earlier example of a 10 Gbps link and a 64-byte IP datagram).
Thus, not only must lookup be performed in hardware, but techniques beyond a simple
linear search through a large table are needed; surveys of fast lookup algorithms can be
found in [Gupta 2001, Ruiz-Sanchez 2001]. Special attention must also be paid to memory access times, resulting in designs with embedded on-chip DRAM and faster SRAM
(used as a DRAM cache) memories. Ternary Content Address Memories (TCAMs) are
also often used for lookup. With a TCAM, a 32-bit IP address is presented to the memory, which returns the content of the forwarding table entry for that address in essentially constant time. The Cisco 8500 has a 64K CAM for each input port.
Once a packet’s output port has been determined via the lookup, the packet can
be sent into the switching fabric. In some designs, a packet may be temporarily
blocked from entering the switching fabric if packets from other input ports are currently using the fabric. A blocked packet will be queued at the input port and then
scheduled to cross the fabric at a later point in time. We’ll take a closer look at the
blocking, queuing, and scheduling of packets (at both input ports and output ports)
in Section 4.3.4. Although “lookup” is arguably the most important action in input
port processing, many other actions must be taken: (1) physical- and link-layer processing must occur, as discussed above; (2) the packet’s version number, checksum
and time-to-live field—all of which we’ll study in Section 4.4.1—must be checked
and the latter two fields rewritten; and (3) counters used for network management
(such as the number of IP datagrams received) must be updated.
Let’s close our discussion of input port processing by noting that the input port
steps of looking up an IP address (“match”) then sending the packet into the switching
fabric (“action”) is a specific case of a more general “match plus action” abstraction
that is performed in many networked devices, not just routers. In link-layer switches
(covered in Chapter 5), link-layer destination addresses are looked up and several
actions may be taken in addition to sending the frame into the switching fabric towards
the output port. In firewalls (covered in Chapter 8)—devices that filter out selected
incoming packets—an incoming packet whose header matches a given criteria (e.g., a
combination of source/destination IP addresses and transport-layer port numbers) may
be prevented from being forwarded (action). In a network address translator (NAT, covered in Section 4.4), an incoming packet whose transport-layer port number matches a
given value will have its port number rewritten before forwarding (action). Thus, the
“match plus action” abstraction is both powerful and prevalent in network devices.
4.3.2 Switching
The switching fabric is at the very heart of a router, as it is through this fabric that
the packets are actually switched (that is, forwarded) from an input port to an output
port. Switching can be accomplished in a number of ways, as shown in Figure 4.8:
• Switching via memory. The simplest, earliest routers were traditional computers,
with switching between input and output ports being done under direct control of
324 CHAPTER 4 • THE NETWORK LAYER
the CPU (routing processor). Input and output ports functioned as traditional I/O
devices in a traditional operating system. An input port with an arriving packet
first signaled the routing processor via an interrupt. The packet was then copied
from the input port into processor memory. The routing processor then extracted
the destination address from the header, looked up the appropriate output port in
the forwarding table, and copied the packet to the output port’s buffers. In this
scenario, if the memory bandwidth is such that B packets per second can be written into, or read from, memory, then the overall forwarding throughput (the total
rate at which packets are transferred from input ports to output ports) must be
less than B/2. Note also that two packets cannot be forwarded at the same time,
even if they have different destination ports, since only one memory read/write
over the shared system bus can be done at a time.
Many modern routers switch via memory. A major difference from early routers,
however, is that the lookup of the destination address and the storing of the packet
into the appropriate memory location are performed by processing on the input
line cards. In some ways, routers that switch via memory look very much like
shared-memory multiprocessors, with the processing on a line card switching
(writing) packets into the memory of the appropriate output port. Cisco’s Catalyst
8500 series switches [Cisco 8500 2012] forward packets via a shared memory.
4.3 • WHAT’S INSIDE A ROUTER? 325
Memory
A B C
X Y Z
Memory
Key:
Input port Output port
A
X Y Z
B C
Crossbar
A B C
X Y Z
Bus
Figure 4.8  Three switching techniques
• Switching via a bus. In this approach, an input port transfers a packet directly to the
output port over a shared bus, without intervention by the routing processor. This is
typically done by having the input port pre-pend a switch-internal label (header) to
the packet indicating the local output port to which this packet is being transferred
and transmitting the packet onto the bus. The packet is received by all output ports,
but only the port that matches the label will keep the packet. The label is then
removed at the output port, as this label is only used within the switch to cross the
bus. If multiple packets arrive to the router at the same time, each at a different input
port, all but one must wait since only one packet can cross the bus at a time. Because
every packet must cross the single bus, the switching speed of the router is limited
to the bus speed; in our roundabout analogy, this is as if the roundabout could only
contain one car at a time. Nonetheless, switching via a bus is often sufficient for
routers that operate in small local area and enterprise networks. The Cisco 5600
[Cisco Switches 2012] switches packets over a 32 Gbps backplane bus.
• Switching via an interconnection network. One way to overcome the bandwidth
limitation of a single, shared bus is to use a more sophisticated interconnection network, such as those that have been used in the past to interconnect processors in a
multiprocessor computer architecture. A crossbar switch is an interconnection network consisting of 2N buses that connect N input ports to N output ports, as shown
in Figure 4.8. Each vertical bus intersects each horizontal bus at a crosspoint, which
can be opened or closed at any time by the switch fabric controller (whose logic is
part of the switching fabric itself). When a packet arrives from port A and needs to
be forwarded to port Y, the switch controller closes the crosspoint at the intersection
of busses A and Y, and port A then sends the packet onto its bus, which is picked up
(only) by bus Y. Note that a packet from port B can be forwarded to port X at the
same time, since the A-to-Y and B-to-X packets use different input and output
busses. Thus, unlike the previous two switching approaches, crossbar networks are
capable of forwarding multiple packets in parallel. However, if two packets from
two different input ports are destined to the same output port, then one will have to
wait at the input, since only one packet can be sent over any given bus at a time.
More sophisticated interconnection networks use multiple stages of switching
elements to allow packets from different input ports to proceed towards the same
output port at the same time through the switching fabric. See [Tobagi 1990] for
a survey of switch architectures. Cisco 12000 family switches [Cisco 12000
2012] use an interconnection network.
4.3.3 Output Processing
Output port processing, shown in Figure 4.9, takes packets that have been stored in
the output port’s memory and transmits them over the output link. This includes
selecting and de-queueing packets for transmission, and performing the needed linklayer and physical-layer transmission functions.
326 CHAPTER 4 • THE NETWORK LAYER
4.3.4 Where Does Queueing Occur?
If we consider input and output port functionality and the configurations shown in
Figure 4.8, it’s clear that packet queues may form at both the input ports and the output ports, just as we identified cases where cars may wait at the inputs and outputs of
the traffic intersection in our roundabout analogy. The location and extent of queueing
(either at the input port queues or the output port queues) will depend on the traffic
load, the relative speed of the switching fabric, and the line speed. Let’s now consider
these queues in a bit more detail, since as these queues grow large, the router’s memory can eventually be exhausted and packet loss will occur when no memory is available to store arriving packets. Recall that in our earlier discussions, we said that
packets were “lost within the network” or “dropped at a router.” It is here, at these
queues within a router, where such packets are actually dropped and lost.
Suppose that the input and output line speeds (transmission rates) all have an
identical transmission rate of R
line packets per second, and that there are N input
ports and N output ports. To further simplify the discussion, let’s assume that all
packets have the same fixed length, and the packets arrive to input ports in a synchronous manner. That is, the time to send a packet on any link is equal to the time
to receive a packet on any link, and during such an interval of time, either zero or
one packet can arrive on an input link. Define the switching fabric transfer rate
R
switch as the rate at which packets can be moved from input port to output port. If
R
switch is N times faster than Rline, then only negligible queuing will occur at the
input ports. This is because even in the worst case, where all N input lines are
receiving packets, and all packets are to be forwarded to the same output port, each
batch of N packets (one packet per input port) can be cleared through the switch fabric before the next batch arrives.
But what can happen at the output ports? Let’s suppose that Rswitch is still N
times faster than R
line. Once again, packets arriving at each of the N input ports
are destined to the same output port. In this case, in the time it takes to send a single
packet onto the outgoing link, N new packets will arrive at this output port. Since
the output port can transmit only a single packet in a unit of time (the packet transmission time), the N arriving packets will have to queue (wait) for transmission over
the outgoing link. Then N more packets can possibly arrive in the time it takes to
4.3 • WHAT’S INSIDE A ROUTER? 327
Line
termination
Data link
processing
(protocol,
encapsulation)
Queuing (buffer
Switch management)
fabric
Figure 4.9  Output port processing
transmit just one of the N packets that had just previously been queued. And so on.
Eventually, the number of queued packets can grow large enough to exhaust available memory at the output port, in which case packets are dropped.
Output port queuing is illustrated in Figure 4.10. At time t, a packet has arrived at
each of the incoming input ports, each destined for the uppermost outgoing port.
Assuming identical line speeds and a switch operating at three times the line speed,
one time unit later (that is, in the time needed to receive or send a packet), all three
original packets have been transferred to the outgoing port and are queued awaiting
transmission. In the next time unit, one of these three packets will have been transmitted over the outgoing link. In our example, two new packets have arrived at the incoming side of the switch; one of these packets is destined for this uppermost output port.
Given that router buffers are needed to absorb the fluctuations in traffic load, the
natural question to ask is how much buffering is required. For many years, the rule of
thumb [RFC 3439] for buffer sizing was that the amount of buffering (B) should be
equal to an average round-trip time (RTT, say 250 msec) times the link capacity (C).
This result is based on an analysis of the queueing dynamics of a relatively small number of TCP flows [Villamizar 1994]. Thus, a 10 Gbps link with an RTT of 250 msec
would need an amount of buffering equal to B = RTT · C = 2.5 Gbits of buffers. Recent
328 CHAPTER 4 • THE NETWORK LAYER
Switch
fabric
Output port contention at time t
One packet time later
Switch
fabric
Figure 4.10  Output port queuing
theoretical and experimental efforts [Appenzeller 2004], however, suggest that when
there are a large number of TCP flows (N) passing through a link, the amount of buffering needed is B = RTT  C/v— N. With a large number of flows typically passing through
large backbone router links (see, e.g., [Fraleigh 2003]), the value of N can be large, with
the decrease in needed buffer size becoming quite significant. [Appenzellar 2004; Wischik 2005; Beheshti 2008] provide very readable discussions of the buffer sizing problem from a theoretical, implementation, and operational standpoint.
A consequence of output port queuing is that a packet scheduler at the output
port must choose one packet among those queued for transmission. This selection
might be done on a simple basis, such as first-come-first-served (FCFS) scheduling,
or a more sophisticated scheduling discipline such as weighted fair queuing (WFQ),
which shares the outgoing link fairly among the different end-to-end connections
that have packets queued for transmission. Packet scheduling plays a crucial role in
providing quality-of-service guarantees. We’ll thus cover packet scheduling extensively in Chapter 7. A discussion of output port packet scheduling disciplines is
[Cisco Queue 2012].
Similarly, if there is not enough memory to buffer an incoming packet, a decision
must be made to either drop the arriving packet (a policy known as drop-tail) or
remove one or more already-queued packets to make room for the newly arrived
packet. In some cases, it may be advantageous to drop (or mark the header of) a packet
before the buffer is full in order to provide a congestion signal to the sender. A number
of packet-dropping and -marking policies (which collectively have become known as
active queue management (AQM) algorithms) have been proposed and analyzed
[Labrador 1999, Hollot 2002]. One of the most widely studied and implemented AQM
algorithms is the Random Early Detection (RED) algorithm. Under RED, a
weighted average is maintained for the length of the output queue. If the average
queue length is less than a minimum threshold, minth, when a packet arrives, the
packet is admitted to the queue. Conversely, if the queue is full or the average queue
length is greater than a maximum threshold, maxth, when a packet arrives, the packet
is marked or dropped. Finally, if the packet arrives to find an average queue length in
the interval [minth, maxth], the packet is marked or dropped with a probability that is
typically some function of the average queue length, minth, and maxth. A number of
probabilistic marking/dropping functions have been proposed, and various versions of
RED have been analytically modeled, simulated, and/or implemented. [Christiansen
2001] and [Floyd 2012] provide overviews and pointers to additional reading.
4.3 • WHAT’S INSIDE A ROUTER? 329
If the switch fabric is not fast enough (relative to the input line speeds) to transfer
all arriving packets through the fabric without delay, then packet queuing can also
occur at the input ports, as packets must join input port queues to wait their turn to be
transferred through the switching fabric to the output port. To illustrate an important
consequence of this queuing, consider a crossbar switching fabric and suppose that
(1) all link speeds are identical, (2) that one packet can be transferred from any one
input port to a given output port in the same amount of time it takes for a packet to be
received on an input link, and (3) packets are moved from a given input queue to their
desired output queue in an FCFS manner. Multiple packets can be transferred in parallel, as long as their output ports are different. However, if two packets at the front of
two input queues are destined for the same output queue, then one of the packets will
be blocked and must wait at the input queue—the switching fabric can transfer only
one packet to a given output port at a time.
Figure 4.11 shows an example in which two packets (darkly shaded) at the front
of their input queues are destined for the same upper-right output port. Suppose that
the switch fabric chooses to transfer the packet from the front of the upper-left
queue. In this case, the darkly shaded packet in the lower-left queue must wait. But
not only must this darkly shaded packet wait, so too must the lightly shaded packet
that is queued behind that packet in the lower-left queue, even though there is no
contention for the middle-right output port (the destination for the lightly shaded
packet). This phenomenon is known as head-of-the-line (HOL) blocking in an
330 CHAPTER 4 • THE NETWORK LAYER
Switch
fabric
Output port contention at time t —
one dark packet can be transferred
Light blue packet experiences HOL blocking
Switch
fabric
Key:
destined for upper output
port
destined for middle output
port
destined for lower output
port
Figure 4.11  HOL blocking at an input queued switch
input-queued switch—a queued packet in an input queue must wait for transfer
through the fabric (even though its output port is free) because it is blocked by
another packet at the head of the line. [Karol 1987] shows that due to HOL blocking, the input queue will grow to unbounded length (informally, this is equivalent to
saying that significant packet loss will occur) under certain assumptions as soon as
the packet arrival rate on the input links reaches only 58 percent of their capacity. A
number of solutions to HOL blocking are discussed in [McKeown 1997b].
4.3.5 The Routing Control Plane
In our discussion thus far and in Figure 4.6, we’ve implicitly assumed that the routing control plane fully resides and executes in a routing processor within the router.
The network-wide routing control plane is thus decentralized—with different pieces
(e.g., of a routing algorithm) executing at different routers and interacting by sending control messages to each other. Indeed, today’s Internet routers and the routing
algorithms we’ll study in Section 4.6 operate in exactly this manner. Additionally,
router and switch vendors bundle their hardware data plane and software control
plane together into closed (but inter-operable) platforms in a vertically integrated
product.
Recently, a number of researchers [Caesar 2005a, Casado 2009, McKeown
2008] have begun exploring new router control plane architectures in which part of
the control plane is implemented in the routers (e.g., local measurement/reporting of
link state, forwarding table installation and maintenance) along with the data plane,
and part of the control plane can be implemented externally to the router (e.g., in a
centralized server, which could perform route calculation). A well-defined API dictates how these two parts interact and communicate with each other. These
researchers argue that separating the software control plane from the hardware data
plane (with a minimal router-resident control plane) can simplify routing by replacing distributed routing calculation with centralized routing calculation, and enable
network innovation by allowing different customized control planes to operate over
fast hardware data planes.
4.4 The Internet Protocol (IP): Forwarding and
Addressing in the Internet
Our discussion of network-layer addressing and forwarding thus far has been
without reference to any specific computer network. In this section, we’ll turn our
attention to how addressing and forwarding are done in the Internet. We’ll see that
Internet addressing and forwarding are important components of the Internet
Protocol (IP). There are two versions of IP in use today. We’ll first examine the
widely deployed IP protocol version 4, which is usually referred to simply as IPv4
4.4 • THE INTERNET PROTOCOL (IP) 331
[RFC 791]. We’ll examine IP version 6 [RFC 2460; RFC 4291], which has been
proposed to replace IPv4, at the end of this section.
But before beginning our foray into IP, let’s take a step back and consider the
components that make up the Internet’s network layer. As shown in Figure 4.12,
the Internet’s network layer has three major components. The first component is
the IP protocol, the topic of this section. The second major component is the routing component, which determines the path a datagram follows from source to destination. We mentioned earlier that routing protocols compute the forwarding
tables that are used to forward packets through the network. We’ll study the
Internet’s routing protocols in Section 4.6. The final component of the network
layer is a facility to report errors in datagrams and respond to requests for certain
network-layer information. We’ll cover the Internet’s network-layer error- and
information-reporting protocol, the Internet Control Message Protocol (ICMP), in
Section 4.4.3.
4.4.1 Datagram Format
Recall that a network-layer packet is referred to as a datagram. We begin our study
of IP with an overview of the syntax and semantics of the IPv4 datagram. You
might be thinking that nothing could be drier than the syntax and semantics of a
packet’s bits. Nevertheless, the datagram plays a central role in the Internet—every
networking student and professional needs to see it, absorb it, and master it. The
332 CHAPTER 4 • THE NETWORK LAYER
Routing protocols
• path selection
• RIP, OSPF, BGP
IP protocol
• addressing conventions
• datagram format
• packet handling
conventions
ICMP protocol
• error reporting
• router “signaling”
Forwarding
table
Transport layer: TCP, UDP
Link layer
Physical layer
Network layer
Figure 4.12  A look inside the Internet’s network layer
IPv4 datagram format is shown in Figure 4.13. The key fields in the IPv4 datagram
are the following:
• Version number. These 4 bits specify the IP protocol version of the datagram.
By looking at the version number, the router can determine how to interpret
the remainder of the IP datagram. Different versions of IP use different datagram formats. The datagram format for the current version of IP, IPv4, is
shown in Figure 4.13. The datagram format for the new version of IP (IPv6) is
discussed at the end of this section.
• Header length. Because an IPv4 datagram can contain a variable number of
options (which are included in the IPv4 datagram header), these 4 bits are needed
to determine where in the IP datagram the data actually begins. Most IP datagrams do not contain options, so the typical IP datagram has a 20-byte header.
• Type of service. The type of service (TOS) bits were included in the IPv4 header
to allow different types of IP datagrams (for example, datagrams particularly
requiring low delay, high throughput, or reliability) to be distinguished from each
other. For example, it might be useful to distinguish real-time datagrams (such as
those used by an IP telephony application) from non-real-time traffic (for example, FTP). The specific level of service to be provided is a policy issue determined by the router’s administrator. We’ll explore the topic of differentiated
service in Chapter 7.
4.4 • THE INTERNET PROTOCOL (IP) 333
Version Type of service Header
length
Upper-layer
protocol
16-bit Identifier
Time-to-live
Flags 13-bit Fragmentation offset
Datagram length (bytes)
Header checksum
32 bits
32-bit Source IP address
32-bit Destination IP address
Options (if any)
Data
Figure 4.13  IPv4 datagram format
• Datagram length. This is the total length of the IP datagram (header plus data),
measured in bytes. Since this field is 16 bits long, the theoretical maximum size
of the IP datagram is 65,535 bytes. However, datagrams are rarely larger than
1,500 bytes.
• Identifier, flags, fragmentation offset. These three fields have to do with so-called
IP fragmentation, a topic we will consider in depth shortly. Interestingly, the new
version of IP, IPv6, does not allow for fragmentation at routers.
• Time-to-live. The time-to-live (TTL) field is included to ensure that datagrams
do not circulate forever (due to, for example, a long-lived routing loop) in the
network. This field is decremented by one each time the datagram is processed
by a router. If the TTL field reaches 0, the datagram must be dropped.
• Protocol. This field is used only when an IP datagram reaches its final destination. The value of this field indicates the specific transport-layer protocol to
which the data portion of this IP datagram should be passed. For example, a
value of 6 indicates that the data portion is passed to TCP, while a value of 17
indicates that the data is passed to UDP. For a list of all possible values, see
[IANA Protocol Numbers 2012]. Note that the protocol number in the IP datagram has a role that is analogous to the role of the port number field in the transportlayer segment. The protocol number is the glue that binds the network and transport
layers together, whereas the port number is the glue that binds the transport and
application layers together. We’ll see in Chapter 5 that the link-layer frame also
has a special field that binds the link layer to the network layer.
• Header checksum. The header checksum aids a router in detecting bit errors in a
received IP datagram. The header checksum is computed by treating each 2 bytes
in the header as a number and summing these numbers using 1s complement
arithmetic. As discussed in Section 3.3, the 1s complement of this sum, known
as the Internet checksum, is stored in the checksum field. A router computes the
header checksum for each received IP datagram and detects an error condition if
the checksum carried in the datagram header does not equal the computed checksum. Routers typically discard datagrams for which an error has been detected.
Note that the checksum must be recomputed and stored again at each router, as
the TTL field, and possibly the options field as well, may change. An interesting
discussion of fast algorithms for computing the Internet checksum is [RFC
1071]. A question often asked at this point is, why does TCP/IP perform error
checking at both the transport and network layers? There are several reasons for
this repetition. First, note that only the IP header is checksummed at the IP layer,
while the TCP/UDP checksum is computed over the entire TCP/UDP segment.
Second, TCP/UDP and IP do not necessarily both have to belong to the same protocol stack. TCP can, in principle, run over a different protocol (for example,
ATM) and IP can carry data that will not be passed to TCP/UDP.
• Source and destination IP addresses. When a source creates a datagram, it inserts
its IP address into the source IP address field and inserts the address of the
334 CHAPTER 4 • THE NETWORK LAYER
ultimate destination into the destination IP address field. Often the source host
determines the destination address via a DNS lookup, as discussed in Chapter 2.
We’ll discuss IP addressing in detail in Section 4.4.2.
• Options. The options fields allow an IP header to be extended. Header options
were meant to be used rarely—hence the decision to save overhead by not
including the information in options fields in every datagram header. However,
the mere existence of options does complicate matters—since datagram headers
can be of variable length, one cannot determine a priori where the data field will
start. Also, since some datagrams may require options processing and others may
not, the amount of time needed to process an IP datagram at a router can vary
greatly. These considerations become particularly important for IP processing in
high-performance routers and hosts. For these reasons and others, IP options
were dropped in the IPv6 header, as discussed in Section 4.4.4.
• Data (payload). Finally, we come to the last and most important field—the raison d’être for the datagram in the first place! In most circumstances, the data
field of the IP datagram contains the transport-layer segment (TCP or UDP) to
be delivered to the destination. However, the data field can carry other types of
data, such as ICMP messages (discussed in Section 4.4.3).
Note that an IP datagram has a total of 20 bytes of header (assuming no options). If
the datagram carries a TCP segment, then each (nonfragmented) datagram carries a
total of 40 bytes of header (20 bytes of IP header plus 20 bytes of TCP header) along
with the application-layer message.